# 数学之美
2017-05


## 第 1 章  文字和语言 vs 数字和信息
1798 年，在一个叫罗塞塔（Rosetta）的西方，发现了一块破碎的古埃及石碑，上面有三种语言：埃及象形文字、埃及拼音文字和古希腊文。1822 年法国语言学家商博良（Champollion）破解了罗塞塔石碑上的古埃及象形文字。让我们了解了整个埃及从公元前 32 世纪至今的历史

罗马数字：
小数字出现在大数左边为减，右边为加。如 IV 表示 5 - 1 = 4，VII 表示 5 + 2 = 7， IIXX 表示 20 - 2 = 18。

描述数字最有效的是古印度人，他们发明了包括 0 在内的 10 个阿拉伯数字。它由阿拉伯人传入欧洲，马上得到了普及。

从象形文字到拼音文字是一个飞跃，因为人类在描述物体的方式上，从物体的外表进化到了抽象的概念，同时不自觉地采用了对信息的编码。


## 第 2 章  自然语言处理 —— 从规则到统计
自然语言处理 60 多年的发展过程，基本可以分成两个阶段。早期的 20 多年，从 20 世纪 50 年代到 70 年代，是科学家们走弯路的阶段。全世界的科学家对计算机处理自然语言的认识都局限在人类学习语言的方式上，也就是说，用电脑模拟人脑，这 20 多年的成果近乎为 0。之后，找到了给予数学模型和统计的方法，自然语言处理进入第二个阶段。

自然语言在演变过程中，产生了词义和上下文相关的特性。因此，它的文法是比较复杂的上下文有关文法，而程序语言是我们人为设计的、便于计算机解码的上下文无关文法，相比自然语言简单得多。理解两者的计算量不可同日而语。

对于上下文无关文法，算法的复杂度基本是语句长度的 2 次方，而对于上下文有关文法，计算复杂度基本上是语句长度的 6 次方。也就是说，长度同为 10 的程序语言和自然语言的语句，计算机对它们进行文法分析的计算量，前者是后者的一万倍。


## 第 3 章  统计语言模型
马尔可夫假设：
假设任意一个词 w(i) 出现的概率只同它前面的词 w(i-1) 有关。

条件概率：S 这个序列出现的概率等于每一个词出现的条件概率相乘
P(S) = P(w1,w2,...,wn) = P(w1)·P(w2|w1)···P(wn|w1,w2,...,wn-1)

N 元模型：
假设一个词由前面的 N-1 个词决定。
N 元模型的时间和空间复杂度都是 N 的指数函数，底数是词典的词汇量，一般在几万到几十万。因此 N 不能很大。N > 3 后效果提升就不是很显著了。

大数定理：
只要统计量足够，相对频度就等于概率。

古德 - 图灵估计：
训练统计语言模型的艺术在于解决好统计样本不足时的概率估计问题。在统计中相信可靠的统计数据，而对不可信的统计数据打折扣的一种概率估计方法，同时将折扣出来的那一小部分概率给予未看见的事件。
对于频率超过一定阈值的词，它们的概率估计就是相对频度，对于频率小于这个阈值的词，它们的概率估计就小于它们的相对频度，出现次数越少的，折扣越多。对于未看见的词，也给予一个比较小的概率，而非零概率。


## 第 4 章  谈谈分词
统计语言模型很大程度上是依照 “大众的想法”，或者 “多数句子的用法”，而在特定情况下可能是错的。


## 第 5 章  隐含马尔可夫模型
马尔可夫假设：
随机过程中各个状态 s(t) 的概率分布，只与它的前一个状态 s(t-1) 有关，即 P(st|s1,s2,...,st-1) = P(st/st-1)。

马尔可夫链：
符合马尔可夫假设的随机过程称为马尔可夫过程，即：马尔可夫链。

隐含马尔可夫模型：
它是马尔可夫链的一个扩展：任一时刻 t 的状态 s(t) 是不可见的。但模型每个时刻 t 会输出一个可见状态 o(t)，而且 o(t) 跟 s(t) 相关且仅跟跟 s(t) 相关。这个称为独立输出假设。
> 在语音识别中，输出的可见状态 o(t) 相当于每个 t 的语音，而需要求解 s(t) 的文字，计算得到一个 P(s1,s2,...,st) 的最大可能概率，就能识别出最可能的原始文字序列


## 第 6 章  信息的度量和作用
信息熵：
一本 50 万字的中文书平均有多少信息量。常用汉子（一二级国标）有 7000 字。假如每个字等概率，那么需要 13 bit（2^13 = 8192）表示一个汉字。但汉字的使用频率是不均等的。实际上，前 10% 的汉字占常用文本的 95% 以上。考虑概率和上下文相关性，每个汉字的信息熵只有 5 bit 左右。所以一本 50 万字的中文书，信息量大约是 5 * 50w = 250w bit。
这里的 250 万 bit 是个平均数，同样长度的书，所含的信息量可以相差很多。如果一本书重复的内容很多，它的信息量就小，冗余度就大。

互信息：
作为两个随机事件 “相关性” 的量化度量。
> 自然语言处理中，用词的互信息（共同出现）来解决翻译的二义性问题。


## 第 7 章  贾里尼克和现代语言处理
> 关于作者导师，自然语言处理之父的故事和观点

教育观点：

  1. 小学生和中学生没必要花那么多时间读书，而他们的社会经验、生活能力以及在那时树立起的志向将帮助他们的一生。
  2. 中学阶段花很多时间比同伴多读的课程，上大学后用很短时间就能读完，因为在大学阶段，人的理解力要强的多。
  3. 学习和教育是持续一辈子的过程。
  4. 书本的内容可以早学，也可以晚学，但是错过了成长阶段却是无法补回来的，因此少年班的做法不足取。

在贾里尼克之前，科学家们把语音识别问题当作人工智能和模式匹配问题。而贾里尼克把它当成通信问题，并用两个隐含马尔可夫模型（声学模型和语言模型）把语音识别概括得清清楚楚。这个框架结构至今仍对语音和语言处理影响深远。它不仅从根本上使得语音识别有实用的可能，而且奠定了今天自然语言处理的基础。

贾里尼克教授在学术上给我最大的帮助就是提高了我在学术上的境界。他告诉我最多的是：什么方法不好。在这一点上与股神巴菲特给和他吃饭的投资人的建议有异曲同工之处。巴菲特和那些投资人讲，你们都非常聪明，不需要我告诉你们做什么，我只需要告诉你们不要去做什么。


## 第 8 章  简单之美 —— 布尔代数和搜索引擎
> 搜索引擎的索引原理

搜索产品都是提炼成下载、索引和排序这三种基本服务，这就是搜索的“道”。
搜索引擎的索引就变成了一张大表：表的每一行对应一个关键词，而每一个关键词后面跟着一组数字，是包含该关键词的文献序号。


## 第 9 章  图论和网络爬虫
> 爬虫背后的数学原理 —— 图论

欧拉定理：如果一个图能够从一个顶点出发，每条边不重复地遍历一遍回到这个顶点，那么每一顶点的度必须为偶数。
度：对于图中的每一个顶点，将与之相连的边的数量定义为它的度。


## 第 10 章  PageRank —— Google 的民主表决式网页排名技术
> PageRank 决定网页质量排名

如果一个网页被很多其他网页所链接，说明它受到普遍的承认和信赖，那么它的排名就高，这就是 PageRank 的核心思想。
对来自不同网页的链接区别对待，因为那些排名高的网页链接更可靠，于是要给这些链接比较大的权重。
现在麻烦来了，计算搜索结果的网页排名过程中需要用到网页本身的排名，这不成了“先有鸡还是先有蛋”的问题了吗？


## 第 11 章  如何确定网页和查询的相关性
> TF-IDF 决定网页与查询的相关性

TF：Term Frequency 单文本词频
IDF：Inverse Document Frequency 逆文本频率指数，公式是 log(D/Dw)，其中 D 是全部网页数，Dw 是关键词在 Dw 个网页中出现的次数。

Query：原子能的应用，网页有 1000 词，其中 “原子能”、“的”、“应用” 各出现 2 次，35 次和 5 次，那么 TF 分别是 0.002、0.035、0.005。
其中 “的” 这种词没什么用，称为 “停止词（Stop Word）”，汉语中，停止词还有 “是”、“和”、“中”、“地”、“得” 等几十个。
一个词预测主题的能力越强，权重越大，而停止词的权重为 0。
很容易发现，如果一个关键词只在很少的网页中出现，通过它就很容易锁定搜索目标，它的权重也就应该越大。反之，如果一个词在大量网页中出现，看到它仍然不很清楚要找什么内容，它的权重就应该小。

权重就是 IDF，那么网页相关性就是：R = TF1·IDF1 + TF2·IDF2 + ··· + TFn·IDFn。
那么给定一个查询，有关网页的综合排名大致由相关性 R 和网页质量排名 PageRank 的乘积决定。


## 第 12 章  有限状态机和动态规划
有限状态机：地址识别
动态规划：地图应用，最短路径
二者结合：语音识别与自然语言理解，加权的有限状态机
> 描述一句话的语音识别结果，就是通过在一张由词构成的概率路径图上找出概率最大的那条路径。


## 第 13 章  Google AK-47 的设计者 —— 阿米特·辛格博士
> AK47 比喻 Google 内部排序算法

在工程上简单实用的方法最好，先帮助用户解决 80% 的问题，再满满解决剩下的 20% 问题。


## 第 14 章  余弦定理和新闻的分类
这种新闻分类的方法，准确性很好，适用于被分类的文本集合在百万数量级。如果大到亿这个数量级，那么计算时间还是比较长的。


## 第 15 章  矩阵运算和文本处理的两个分类问题
余弦定理算法非常漂亮，但是因为要对所有新闻做两两计算，而且要进行很多次迭代，耗时会特别长，尤其是当新闻的数量很大且词表也很大的时候。我们希望有一个办法，一次就能把所有新闻相关性计算出来。这个一步到位的办法利用的是矩阵运算中的奇异值分解（SVD：Singular Value Decomposition）。

奇异值分解，就是把一个大矩阵，分解成三个小矩阵相乘。
奇异值分解的优点是能较快的得到结果，但分类的结果略显粗糙，因此，它适合处理超大规模文本的粗分类。在实际应用中，可以先进行奇异值分解，得到粗分类结果，再利用计算向量余弦的方法，在粗分类结果的基础上，进行几次迭代，得到比较精确的结果。这两个方法一先一后结合使用，可以充分利用两者的优势，既节省时间，又能获得很好的准确性。


## 第 16 章  信息指纹及其应用
任何一段信息（包括文字、语音、视频、图片等），都可以对应一个不太长的随机数，作为区别这段信息和其他信息的指纹。
信息指纹的一个特征是不可逆性，用于：网址消重，密码。
信息指纹算法：伪随机数产生器算法，通过它将任意很长的整数转换成特定长度的伪随机数，现在常用是梅森旋转算法（Mersenne Twister）

视频的匹配有两个核心技术，关键帧的提取和特征的提取。
一般来说，每一秒或若干秒才有一帧是完整的图像，这些帧称为关键帧。

相似哈希是一种特殊的信息指纹，其特点是：如果两个网页的相似哈希相差越小，这两个网页的相似性就越高。如果两个网页相同，它们的相似哈希必定相同。如果它们只有少数权重小的词不相同，其余的都相同，几乎可以肯定它们的相似哈希也会相同。值得一提的是，如果两个网页的相似哈希不同，但是相差很小，则对应的网页也非常相似。用 64 位的相似哈希做对比时，如果只相差一两位，那么对应网页内容重复的可能性大于 80%。


## 第 17 章  由电视剧《暗算》所想到的 —— 谈谈密码学的数学原理
密码的最高境界是敌方在截获密码后，对我方的所知没有任何增加，用信息论的专业术语讲，就是信息量没有增加。一般来讲，但密码之间分布均匀并且统计独立时，提供的信息最少。


## 第 18 章  闪光的不一定是金子 —— 谈谈搜索引擎反作弊问题和搜索结果的权威性问题
权威度与一般的网页质量不同，它和要搜索的主题是相关的。权威性的这个特点，即与搜索关键词的相关性。
