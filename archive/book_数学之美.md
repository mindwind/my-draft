# 数学之美
2017-05


## 第 1 章  文字和语言 vs 数字和信息
1798 年，在一个叫罗塞塔（Rosetta）的西方，发现了一块破碎的古埃及石碑，上面有三种语言：埃及象形文字、埃及拼音文字和古希腊文。1822 年法国语言学家商博良（Champollion）破解了罗塞塔石碑上的古埃及象形文字。让我们了解了整个埃及从公元前 32 世纪至今的历史

罗马数字：
小数字出现在大数左边为减，右边为加。如 IV 表示 5 - 1 = 4，VII 表示 5 + 2 = 7， IIXX 表示 20 - 2 = 18。

描述数字最有效的是古印度人，他们发明了包括 0 在内的 10 个阿拉伯数字。它由阿拉伯人传入欧洲，马上得到了普及。

从象形文字到拼音文字是一个飞跃，因为人类在描述物体的方式上，从物体的外表进化到了抽象的概念，同时不自觉地采用了对信息的编码。


## 第 2 章  自然语言处理 —— 从规则到统计
自然语言处理 60 多年的发展过程，基本可以分成两个阶段。早期的 20 多年，从 20 世纪 50 年代到 70 年代，是科学家们走弯路的阶段。全世界的科学家对计算机处理自然语言的认识都局限在人类学习语言的方式上，也就是说，用电脑模拟人脑，这 20 多年的成果近乎为 0。之后，找到了给予数学模型和统计的方法，自然语言处理进入第二个阶段。

自然语言在演变过程中，产生了词义和上下文相关的特性。因此，它的文法是比较复杂的上下文有关文法，而程序语言是我们人为设计的、便于计算机解码的上下文无关文法，相比自然语言简单得多。理解两者的计算量不可同日而语。

对于上下文无关文法，算法的复杂度基本是语句长度的 2 次方，而对于上下文有关文法，计算复杂度基本上是语句长度的 6 次方。也就是说，长度同为 10 的程序语言和自然语言的语句，计算机对它们进行文法分析的计算量，前者是后者的一万倍。


## 第 3 章  统计语言模型
马尔可夫假设：
假设任意一个词 w(i) 出现的概率只同它前面的词 w(i-1) 有关。

条件概率：S 这个序列出现的概率等于每一个词出现的条件概率相乘
P(S) = P(w1,w2,...,wn) = P(w1)·P(w2|w1)···P(wn|w1,w2,...,wn-1)

N 元模型：
假设一个词由前面的 N-1 个词决定。
N 元模型的时间和空间复杂度都是 N 的指数函数，底数是词典的词汇量，一般在几万到几十万。因此 N 不能很大。N > 3 后效果提升就不是很显著了。

大数定理：
只要统计量足够，相对频度就等于概率。

古德 - 图灵估计：
训练统计语言模型的艺术在于解决好统计样本不足时的概率估计问题。在统计中相信可靠的统计数据，而对不可信的统计数据打折扣的一种概率估计方法，同时将折扣出来的那一小部分概率给予未看见的事件。
对于频率超过一定阈值的词，它们的概率估计就是相对频度，对于频率小于这个阈值的词，它们的概率估计就小于它们的相对频度，出现次数越少的，折扣越多。对于未看见的词，也给予一个比较小的概率，而非零概率。


## 第 4 章  谈谈分词
统计语言模型很大程度上是依照 “大众的想法”，或者 “多数句子的用法”，而在特定情况下可能是错的。


## 第 5 章  隐含马尔可夫模型
马尔可夫假设：
随机过程中各个状态 s(t) 的概率分布，只与它的前一个状态 s(t-1) 有关，即 P(st|s1,s2,...,st-1) = P(st/st-1)。

马尔可夫链：
符合马尔可夫假设的随机过程称为马尔可夫过程，即：马尔可夫链。

隐含马尔可夫模型：
它是马尔可夫链的一个扩展：任一时刻 t 的状态 s(t) 是不可见的。但模型每个时刻 t 会输出一个可见状态 o(t)，而且 o(t) 跟 s(t) 相关且仅跟跟 s(t) 相关。这个称为独立输出假设。
> 在语音识别中，输出的可见状态 o(t) 相当于每个 t 的语音，而需要求解 s(t) 的文字，计算得到一个 P(s1,s2,...,st) 的最大可能概率，就能识别出最可能的原始文字序列


## 第 6 章  信息的度量和作用
信息熵：
一本 50 万字的中文书平均有多少信息量。常用汉子（一二级国标）有 7000 字。假如每个字等概率，那么需要 13 bit（2^13 = 8192）表示一个汉字。但汉字的使用频率是不均等的。实际上，前 10% 的汉字占常用文本的 95% 以上。考虑概率和上下文相关性，每个汉字的信息熵只有 5 bit 左右。所以一本 50 万字的中文书，信息量大约是 5 * 50w = 250w bit。
这里的 250 万 bit 是个平均数，同样长度的书，所含的信息量可以相差很多。如果一本书重复的内容很多，它的信息量就小，冗余度就大。

互信息：
作为两个随机事件 “相关性” 的量化度量。
> 自然语言处理中，用词的互信息（共同出现）来解决翻译的二义性问题。


## 第 7 章  贾里尼克和现代语言处理
> 关于作者导师，自然语言处理之父的故事和观点。开创性贡献：利用信息论解决自然语言问题，从 0 到 1。

教育观点：

  1. 小学生和中学生没必要花那么多时间读书，而他们的社会经验、生活能力以及在那时树立起的志向将帮助他们的一生。
  2. 中学阶段花很多时间比同伴多读的课程，上大学后用很短时间就能读完，因为在大学阶段，人的理解力要强的多。
  3. 学习和教育是持续一辈子的过程。
  4. 书本的内容可以早学，也可以晚学，但是错过了成长阶段却是无法补回来的，因此少年班的做法不足取。

在贾里尼克之前，科学家们把语音识别问题当作人工智能和模式匹配问题。而贾里尼克把它当成通信问题，并用两个隐含马尔可夫模型（声学模型和语言模型）把语音识别概括得清清楚楚。这个框架结构至今仍对语音和语言处理影响深远。它不仅从根本上使得语音识别有实用的可能，而且奠定了今天自然语言处理的基础。

贾里尼克教授在学术上给我最大的帮助就是提高了我在学术上的境界。他告诉我最多的是：什么方法不好。在这一点上与股神巴菲特给和他吃饭的投资人的建议有异曲同工之处。巴菲特和那些投资人讲，你们都非常聪明，不需要我告诉你们做什么，我只需要告诉你们不要去做什么。


## 第 8 章  简单之美 —— 布尔代数和搜索引擎
> 搜索引擎的索引原理

搜索产品都是提炼成下载、索引和排序这三种基本服务，这就是搜索的“道”。
搜索引擎的索引就变成了一张大表：表的每一行对应一个关键词，而每一个关键词后面跟着一组数字，是包含该关键词的文献序号。


## 第 9 章  图论和网络爬虫
> 爬虫背后的数学原理 —— 图论

欧拉定理：如果一个图能够从一个顶点出发，每条边不重复地遍历一遍回到这个顶点，那么每一顶点的度必须为偶数。
度：对于图中的每一个顶点，将与之相连的边的数量定义为它的度。


## 第 10 章  PageRank —— Google 的民主表决式网页排名技术
> PageRank 决定网页质量排名

如果一个网页被很多其他网页所链接，说明它受到普遍的承认和信赖，那么它的排名就高，这就是 PageRank 的核心思想。
对来自不同网页的链接区别对待，因为那些排名高的网页链接更可靠，于是要给这些链接比较大的权重。
现在麻烦来了，计算搜索结果的网页排名过程中需要用到网页本身的排名，这不成了“先有鸡还是先有蛋”的问题了吗？


## 第 11 章  如何确定网页和查询的相关性
> TF-IDF 决定网页与查询的相关性

TF：Term Frequency 单文本词频
IDF：Inverse Document Frequency 逆文本频率指数，公式是 log(D/Dw)，其中 D 是全部网页数，Dw 是关键词在 Dw 个网页中出现的次数。

Query：原子能的应用，网页有 1000 词，其中 “原子能”、“的”、“应用” 各出现 2 次，35 次和 5 次，那么 TF 分别是 0.002、0.035、0.005。
其中 “的” 这种词没什么用，称为 “停止词（Stop Word）”，汉语中，停止词还有 “是”、“和”、“中”、“地”、“得” 等几十个。
一个词预测主题的能力越强，权重越大，而停止词的权重为 0。
很容易发现，如果一个关键词只在很少的网页中出现，通过它就很容易锁定搜索目标，它的权重也就应该越大。反之，如果一个词在大量网页中出现，看到它仍然不很清楚要找什么内容，它的权重就应该小。

权重就是 IDF，那么网页相关性就是：R = TF1·IDF1 + TF2·IDF2 + ··· + TFn·IDFn。
那么给定一个查询，有关网页的综合排名大致由相关性 R 和网页质量排名 PageRank 的乘积决定。


## 第 12 章  有限状态机和动态规划
有限状态机：地址识别
动态规划：地图应用，最短路径
二者结合：语音识别与自然语言理解，加权的有限状态机
> 描述一句话的语音识别结果，就是通过在一张由词构成的概率路径图上找出概率最大的那条路径。


## 第 13 章  Google AK-47 的设计者 —— 阿米特·辛格博士
> AK47 比喻 Google 内部排序算法

在工程上简单实用的方法最好，先帮助用户解决 80% 的问题，再满满解决剩下的 20% 问题。


## 第 14 章  余弦定理和新闻的分类
这种新闻分类的方法，准确性很好，适用于被分类的文本集合在百万数量级。如果大到亿这个数量级，那么计算时间还是比较长的。


## 第 15 章  矩阵运算和文本处理的两个分类问题
余弦定理算法非常漂亮，但是因为要对所有新闻做两两计算，而且要进行很多次迭代，耗时会特别长，尤其是当新闻的数量很大且词表也很大的时候。我们希望有一个办法，一次就能把所有新闻相关性计算出来。这个一步到位的办法利用的是矩阵运算中的奇异值分解（SVD：Singular Value Decomposition）。

奇异值分解，就是把一个大矩阵，分解成三个小矩阵相乘。
奇异值分解的优点是能较快的得到结果，但分类的结果略显粗糙，因此，它适合处理超大规模文本的粗分类。在实际应用中，可以先进行奇异值分解，得到粗分类结果，再利用计算向量余弦的方法，在粗分类结果的基础上，进行几次迭代，得到比较精确的结果。这两个方法一先一后结合使用，可以充分利用两者的优势，既节省时间，又能获得很好的准确性。


## 第 16 章  信息指纹及其应用
任何一段信息（包括文字、语音、视频、图片等），都可以对应一个不太长的随机数，作为区别这段信息和其他信息的指纹。
信息指纹的一个特征是不可逆性，用于：网址消重，密码。
信息指纹算法：伪随机数产生器算法，通过它将任意很长的整数转换成特定长度的伪随机数，现在常用是梅森旋转算法（Mersenne Twister）

视频的匹配有两个核心技术，关键帧的提取和特征的提取。
一般来说，每一秒或若干秒才有一帧是完整的图像，这些帧称为关键帧。

相似哈希是一种特殊的信息指纹，其特点是：如果两个网页的相似哈希相差越小，这两个网页的相似性就越高。如果两个网页相同，它们的相似哈希必定相同。如果它们只有少数权重小的词不相同，其余的都相同，几乎可以肯定它们的相似哈希也会相同。值得一提的是，如果两个网页的相似哈希不同，但是相差很小，则对应的网页也非常相似。用 64 位的相似哈希做对比时，如果只相差一两位，那么对应网页内容重复的可能性大于 80%。


## 第 17 章  由电视剧《暗算》所想到的 —— 谈谈密码学的数学原理
密码的最高境界是敌方在截获密码后，对我方的所知没有任何增加，用信息论的专业术语讲，就是信息量没有增加。一般来讲，但密码之间分布均匀并且统计独立时，提供的信息最少。


## 第 18 章  闪光的不一定是金子 —— 谈谈搜索引擎反作弊问题和搜索结果的权威性问题
权威度与一般的网页质量不同，它和要搜索的主题是相关的。权威性的这个特点，即与搜索关键词的相关性。


## 第 19 章  谈谈数学模型的重要性
  - 一个正确的数学模型应当在形式上是简单的（地心说模型太复杂）。
  - 一个正确的模型一开始可能还不如一个精雕细琢过的错误模型来的准确。（日心说一开始不如地心说准确）
  - 大量准确的数据对研发很重要
  - 正确的模型可能受到噪音干扰，而显得不准确


## 第 20 章  不要把鸡蛋放到一个篮子里 —— 谈谈最大熵模型
最大熵原理：  
对一个随机事件的概率分布进行预测时，我们的预测应当满足全部已知的条件，而对未知的情况不要做任何主观假设。在这种情况下，概率分布最均匀，预测的风险最小。

最大熵模型：
按最大熵原理预测概率分布的信息熵最大，它有个一非常简单的形式 —— 指数函数。

> 缺点：计算量很大，需要找到契合的应用场景;
> 应用：对冲基金证券交易概率预测、自然语言处理中的词性标注和句法分析


## 第 21 章  拼音输入法的数学原理
输入法输入汉字的快慢取决于汉字编码的平均长度，就是用击键次数乘以寻找这个键所需时间。

拼音转汉字的算法和寻找最短路径的算法相同，都是动态规划。
> 寻找概率最大的一组汉字组合


## 第 22 章  自然语言处理的教父马库斯和他的优秀弟子们
> 将自然语言处理从基于规则的研究方法转到基于统计的研究方法。发扬光大的贡献，从 1 到 100。

贡献：建立标准语料库，LDC：Linguistic Data Consortium。如今，在自然语言处理方面发表论文，几乎都要提供基于 LDC 语料库的测试结果。


## 第 23 章  布隆过滤器
它只需散列表 1/8 到 1/4 的大小就能解决同样的问题。
它的好处在于快速、省空间，但是有一定的误识别率。常见的补救办法是再建立一个小的白名单，存储那些可能被误判的邮件地址。
它的不足之处就是它可能把不在集合中的元素错判成集合中的元素，这在检验上被称为 “假阳性”。

它背后的数学原理在于两个完全随机的数字相冲突的概率很小，因此，可以在很小的误识别率条件下，用很少的空间存储大量信息。


## 第 24 章  马尔可夫链的扩展 —— 贝叶斯网络
马尔可夫链是贝叶斯网络的特例，而贝叶斯网络是马尔可夫链的推广。
使用贝叶斯网络必须先确定这个网络的拓扑结构，然后还要知道各个状态之间的概率。
利用贝叶斯网络，可以找出近义词和相关的词。
从数学的层面来讲，贝叶斯网络是一个加权的有向图（概率图模型）。


## 第 25 章  条件随机场、文法分析及其他
条件随机场是隐含马尔可夫模型的一种扩展，更广义地讲，条件随机场是一种特殊的概率图模型。
条件随机场是无向图，而贝叶斯网络是有向图。


## 第 26 章  维特比和他的维特比算法
> 应用场景：拼音输入法出汉字的最大可能路径

维特比算法，是针对一个特殊的图 —— 篱笆网络（Lattice）的有向图最短路径问题而提出来的。
汉语中每个无声调的拼音对应 13 个左右的国标汉字


## 第 27 章  上帝的算法 —— 期望最大化算法
> 应用场景：文本的自收敛分类


## 第 28 章  逻辑回归和搜索广告
> 应用场景：搜索广告点击率预估

逻辑回顾模型是一种将影响概率的不同因素结合在一起的指数模型。


## 第 29 章  各个击破算法和 Google 云计算的基础
> 应用场景：MapReduce 大数组归并排序、大矩阵相乘


## 第 30 章  Google 大脑和人工神经网络
人工神经网络
除了借用了生物学上的一些名词，并且做了一些形象的比喻外，人工神经网络和人脑没有半点关系，它本质上是我们前面介绍过的一种有向图，只不过它是一种特殊的有向图。
节点（神经元）的取值是用一个非线性函数计算出来的，这个函数被称为神经元函数。
在人工神经网络中，规定神经元函数只能对输入变量 —— 指向它的节点的值 —— 线性组合的结果进行一次非线性变换。

无论是在计算机科学、通信、生物统计和医学，还是在金融和经济学中，大多数与 “智能” 有点关系的问题，都可以归结为一个在多维空间进行模式分类的问题。
人工神经网络的应用领域：语音识别、机器翻译、人脸图像识别、癌细胞识别、疾病预测和股市预测等。

人工神经网络与贝叶斯网络的差异
人工神经网络，在结构上是完全标准化的，而贝叶斯网络更灵活。
人工神经网络，神经元函数有固定的规则限制，而贝叶斯网络，变量可以组合成任意函数，无限制，增加了灵活性，但也增加了复杂性。
人工神经网络，输出相对孤立，它可以识别一个个字，但是很难处理一个序列。贝叶斯网络更容易考虑上下文前后的相关性，因此可以解码一个输入的序列，比如将一段语音识别成文字。

过去，如果人工神经网络规模小，就干不了什么事情；如果规模大了，计算量又受不了。
“Google 大脑” 说穿了是一种大规模并行处理的人工神经网络。
“Google 大脑” 并不是一个什么都能思考的大脑，而是一个很能计算的人工神经网络。因此，与其说 Google 大脑很聪明，不如说它很能算。


## 第 31 章  大数据的威力 —— 谈谈数据的重要性

## 附录  计算复杂度
问题的大小是衡量计算复杂度时的变量，一般用 N 来表示。而计算量是 N 的一个函数 f(N)。
这个函数的边界可以用数学上的大 O 概念来限制。如果两个函数 f(N) 和 g(N) 在大 O 概念上相同，也就是说当 N 趋近于无穷大时，它们的比值只差一个常数。

如果一个算法的计算量不超过 N 的多项式函数（Polynomial Function），那么称这个算法是多项式函数复杂度的。
如果一个问题存在一个多项式复杂度算法，那么这个问题成为 P 问题（Polynomial 的首字母）；这类问题被认为是计算机可以 “有效” 解决的。
如果一个算法的计算量比 N 的多项式函数还高，这时我们成它为非多项式（Non-Polynomial）问题；比如找到每一步围棋的最佳走法就是这样的问题。
在非多项式问题中，有一类问题即非确定的多项式问题（Nondeterministic Polynomilal）简称 NP 问题。
NP 问题之所以重要的一个原因是现实中的绝大多数问题都是 NP 的。

库克（Stephen Cook）和李文（Leonid Levin）在 NP 问题中发现一个被称为 NPC（NP-Complete）的特殊问题类，
所有的 NP 问题都可以在多项式时间内规约到 NPC 问题，这个发现称为 “库克-李文” 定理。
如果任何一个 NPC 问题找到了多项式算法，那么所有的 NP 问题都可以用这个算法解决了，也即 NP=P 了。

数学在计算机科学中的一个重要作用，就是找到计算复杂度尽可能低的解。对于 NP-Complete 类的问题，找到近似解。
